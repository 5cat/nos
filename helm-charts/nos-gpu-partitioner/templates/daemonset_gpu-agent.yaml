apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    control-plane: nos-controller-manager
    {{- include "gpu-agent.labels" . | nindent 4 }}
  name: {{ include "gpu-agent.fullname" . }}
spec:
  selector:
    matchLabels:
      control-plane: nos-controller-manager
      {{- include "gpu-agent.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        control-plane: nos-controller-manager
        {{- include "gpu-agent.selectorLabels" . | nindent 8 }}
    spec:
      serviceAccountName: {{ include "gpu-agent.fullname" . }}
      nodeSelector:
        nos.nebuly.com/gpu-partitioning: mps
      priorityClassName: system-node-critical
      terminationGracePeriodSeconds: 20
      containers:
        - image: "{{ .Values.gpuAgent.image.repository }}:{{ .Values.gpuAgent.image.tag | default .Chart.AppVersion }}"
          name: {{ include "gpu-agent.fullname" . }}
          imagePullPolicy: {{ .Values.gpuAgent.image.pullPolicy }}
          args:
            - --config={{ include "gpu-agent.configFileName" . }}
            {{- if gt (int .Values.gpuAgent.logLevel) 0 }}
            - --zap-log-level={{ .Values.gpuAgent.logLevel }}
            {{ end }}
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            {{- toYaml .Values.gpuAgent.resources | nindent 12 }}
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: /{{ include "gpu-agent.configFileName" . }}
              name: gpu-agent-config
              subPath: {{ include "gpu-agent.configFileName" . }}
            - mountPath: /var/lib/kubelet/pod-resources/kubelet.sock
              name: device-plugin
            - mountPath: /run/nvidia
              mountPropagation: HostToContainer
              name: run-nvidia

        - name: kube-rbac-proxy
          args:
            - --secure-listen-address=0.0.0.0:8443
            - --upstream=http://127.0.0.1:8080/
            - --logtostderr=true
            {{- if gt (int .Values.kubeRbacProxy.logLevel) 0 }}
            - --v={{ .Values.kubeRbacProxy.logLevel }}
            {{ end }}
          image: "{{ .Values.kubeRbacProxy.image.repository }}:{{ .Values.kubeRbacProxy.image.tag }}"
          ports:
            - containerPort: 8443
              name: https
              protocol: TCP
          resources: {{- toYaml .Values.kubeRbacProxy.resources | nindent 12 }}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
      tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Exists
          {{- with .Values.gpuAgent.tolerations }}
            {{- toYaml . | nindent 8 }}
          {{- end }}
      volumes:
        - configMap:
            name: {{ include "gpu-agent.config.configMapName" . }}
          name: gpu-agent-config
        - hostPath:
            path: /var/lib/kubelet/pod-resources/kubelet.sock
          name: device-plugin
        - hostPath:
            path: /run/nvidia
            type: Directory
          name: run-nvidia
