apiVersion: config.n8s.nebuly.ai/v1alpha1
kind: GpuPartitionerConfig
health:
  healthProbeBindAddress: :8081
metrics:
  bindAddress: 127.0.0.1:8080
webhook:
  port: 9443
leaderElection:
  leaderElect: true
  resourceName: gpu-partitioner.nebuly.ai
# leaderElectionReleaseOnCancel defines if the leader should step down volume
# when the Manager ends. This requires the binary to immediately end when the
# Manager is stopped, otherwise, this setting is unsafe. Setting this significantly
# speeds up voluntary leader transitions as the new leader don't have to wait
# LeaseDuration time first.
# In the default scaffold provided, the program ends immediately after
# the manager stops, so would be fine to enable this option. However,
# if you are doing or is intended to do any operation such as perform cleanups
# after the manager stops then its usage might be unsafe.
  leaderElectionReleaseOnCancel: true

# Timeout of the window used by the GPU partitioner for batching pending Pods.
#
# Higher values make the GPU partitioner will potentially take into account more pending Pods when
# deciding the GPU partitioning plan, but the partitioning will be performed less frequently
batchWindowTimeoutSeconds: 60
# Idle seconds before the GPU partitioner processes the current batch if no new pending Pods are created, and
# the timeout has not been reached.
#
# Higher values make the GPU partitioner will potentially take into account more pending Pods when
# deciding the GPU partitioning plan, but the partitioning will be performed less frequently
batchWindowIdleSeconds: 10

# Optional path to the configuration file of the k8s scheduler used internally by the GPU
# partitioner for simulating Pods scheduling.
#
# Uncomment if you want to use a custom scheduler configuration file, otherwise the GPU partitioner
# will use the default k8s scheduler profile
#schedulerConfigFile: scheduler_config.yaml

# Optional path to the file containing the possible MIG geometries of each known GPU model
knownMigGeometriesFile: known_mig_geometries.yaml

# Namespaced name of the ConfigMap containing the NVIDIA Device Plugin configuration files.
# It must be equal to the value "devicePlugin.config.name" of the Helm chart used for deploying the
# NVIDIA GPU Operator.
devicePluginConfigMap:
  name: nvidia-plugin-configs
  namespace: gpu-operator